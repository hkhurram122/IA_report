
 
 \documentclass[12pt,a4paper]{article}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{geometry}
 \geometry{
 a4paper,
 total={170mm,257mm},
 left=20mm,
 top=20mm,
 }
\usepackage{color}   %May be necessary if you want to color links
\usepackage{hyperref}
\hypersetup{
    colorlinks=true,
    linktoc=all,     
    linkcolor=blue,  
}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{tcolorbox}
\usepackage[utf8]{gensymb}
\usepackage{graphicx}
\usepackage[english]{babel}
\usepackage{subfig}
\usepackage{indentfirst}
\usepackage[font={small,it}]{caption}
\usepackage[keeplastbox]{flushend}
\usepackage{url}
\usepackage{xcolor}
\usepackage{wrapfig}
\usepackage[title]{appendix}
\usepackage{multicol}
\usepackage{float}
 
\title{Introduction to Supervised Learning and Regression \\
 \large HL Mathematics IA}

\author{Hassan Khurram}
\date{\textit{Updated:} December 2020}

\begin{document}

\maketitle

  
\tableofcontents

\newpage

\section{Introduction}

\subsection{Background: Artificial Intelligence}

From emerging technologies in self-driving automobiles to personal assistants on our phones, AI is the future. The book \textit{Life 3.0: Being Human in the Age of Artificial Intelligence} by Max Tegmark motivated me to appreciate the gravity of the ever-expanding capacity of AI. With various perspectives that define our ethical and philosophical bounds on its presence in technology, the book and a series of documentaries captivated my interest in this technology. Since my realization sparked amidst the rise of AI, I seek to pursue a field in machine intelligence.

What does ‘intelligence’ mean in view of artificial life forms? Max Tegmark, an author and physicist at MIT stated that “Intelligence is the ability to achieve complex goals,” in broad terms that concern both artificial and biological abilities. (Tegmark) But how does this relate to mathematics, particularly in the fields of calculus and linear algebra? AI and machine learning stem from optimization concepts in calculus. What was initially a daunting concept became conceptualized as the aim to minimize a function used to increase the accuracy of a neural network, or an algorithm that carries out such ‘learning’. The network achieves the closest values possible to the actual, desired outputs. The grasp of these concepts enabled me to realize the real-world importance of the calculus optimization problems from class.

\subsection{Aim and Rationale}

My exploration aims to use functions to express the cost of the outputs—or the predicted housing prices compared to the actual values of prices—used for training a neural network in \textbf{supervised learning}. Supervised learning is a process where a neural network, or an algorithm that computes an output given input values adjusts according to how different its output is compared to the actual values. It attempts to model a function to relate inputs to outputs using known examples to learn this relationship. So, in this exploration, my models use regression (for continuous variables) with examples of housing prices, where the price is dependent on multiple variables such as lot size, number of rooms, number of bathrooms, and living area. \textbf{Vector calculus} and matrices are used to determine the value of the cost in each ‘iteration’, or example in a sample of housing data. Since data points are shuffled at random, this method is known as \textbf{stochastic gradient descent}, with the aims to show that cost (loss) tends to descend as the number of iterations increases. In other words, the network ‘trains’ to become more accurate in giving outputs closer to the target, or real values of housing prices. (Ng, 2012)

\textit{Note: “Cost” refers to the mean square error, or a function describing the difference between the network’s estimated value and actual price of the houses. “Price” refers to the money value ($\$$) of houses.
}


\section{AI Concepts}

\subsection{Neural Nets}

In artificial intelligence, neural networks—which roughly emulate our understandings of neuroscience related to machine learning—are represented as a layered structure of neurons. Simply, it takes an input of values (in the input layer) for a given number of variables named features and computes the output of the network in the output layer neurons. Mathematically speaking, each layer consists of neurons that store numbers as activations, or the outputs dependent upon the weights and activations of all neurons in a preceding layer (except for input layer neurons).  An activation is a number stored by a single neuron in the network. Also, weights linking the neurons may be thought of as the strengths of the connections, or ‘synapses’ between neurons. First, forward propagation is carried out to obtain the output values from the neural net, which is going from one layer to the next (to the right). (Kriegeskorte, 2019)

\begin{figure}[h]
    \centering
    \includegraphics[width=0.75\textwidth]{Screen Shot 2020-12-23 at 5.59.26 PM}
    \caption{The perceptron: the most basic neural network of two neurons (one in each layer)}
    \label{fig:my_label}
\end{figure}

Note: The superscript notation with parentheses is not an exponent, but used for indexing the neuron’s layer.

\begin{equation}
    a^{\left(L\right)}=\sigma\left(\left(a^{\left(L-1\right)}\right)\left(w^{(L)}\right)+b\right)
\end{equation}

This equation above is used to calculate the weighted sum, or the output expressed in terms of all inputs, or features (variables) in the input layer. In this case, the input, $a^{(L-1)}$, which is the activation of the ‘input layer’ neuron, is multiplied by a weight, $w^{(L)}$, and added by a bias, b. In the figure above, the output of the output layer neuron is $a^{\left(L\right)}$, when the weighted sum is the input of an activation function: this is represented by the $\sigma$ (sigma) symbol, which is later discussed. Hence, ‘learning’/’training’ in machine intelligence is defined as a computer’s manipulation of both the weights and biases of every neuron in the neural net to the most optimal values as parameters. Where forward propagation is used to compute cost (error) values in terms of a training set, backpropagation is used to decrease the output of the cost function C($w_1,\ \ldots,\ w_n$), where there are n number of weights and biases in total for the neural network linking the neurons. (Le, 2015)




\subsection{Activation functions}
Activation functions are applied to the weighted sum outputs, or the activations of neurons in neural nets to a defined range. One activation function will be used in calculations of forward and backpropagation, the latter of which is used for gradient descent, or the process of machine ‘learning’, later explained.

\subsubsection{Sigmoid (Logistic) Function}
The sigmoid activation function is commonly used in neural networks. They are used for the activations of all neurons in classification problems. The rationale pertaining to the use of this function is later explained in detail. The function is defined as  $ y=\ \sigma\left(z\right) $ with the range of $ y\in\left(0,1\right) $. The ‘z’ variable of the sigmoid function is equal to the weighted sum to a single layer.


\begin{figure}[h]
    \centering
    \includegraphics[width=0.3\textwidth]{graph1.eps}
    \caption{$\sigma\left(z\right)=\frac{1}{1+e^{-z}}	$}
    \label{fig:my_label}
\end{figure}

\begin{equation}
\lim_{z \to \infty}{\sigma\left(z\right)=1}
\end{equation}

\begin{equation}
\lim_{z\to -\infty}{\sigma\left(z\right)=0}
\end{equation}

Also, the derivative of this function, which will be used later in partial derivatives in terms of each weight and bias, is:
\begin{equation}
\sigma^\prime\left(z\right)=\frac{e^{-z}}{\left(1+e^{-z}\right)^2}
\end{equation}



\subsection{Gradient Descent Problem}
\subsubsection{Problem and Cost Functions}
The cost function’s value, related to the difference between prediction and actual values, is to be minimized. In single variable calculus, the minimum of a function is graphically represented at the point where the first derivative of the function is equal to zero, and the second derivative is positive, used to directly solve for the local minimum at one point in the plane. However, since the cost function depends on many weights and biases, multivariate calculus (explained later) is used. In this case, a randomized starting point is set before iterative calculations are computed and changes are applied to all the weights and biases in a direction that reduces the cost function’s value closer to a local minimum in space, as the global minimum cannot directly be solved for. Testing data (features) are used as inputs in a neural network to result in output (activation), or prediction compared to desired outputs in a defined cost function, using the mean square loss/cost function used commonly for regression back-propagation. (Strang, 2018)

\begin{equation}
C\left(w_1,\ldots,w_n\right)= \frac{1}{2}\left(a^{\left(L\right)}-y\right)^2
a^{\left(L\right)} = activation \,in\, L
\end{equation}

\textit{y = actual value of the output}

\hspace{}

Here, the mean square loss function is used as errors may be negative, and so the squares of the errors are taken for each output in a data training set to give a non negative value of the error. Also, a factor of $\frac{1}{2}$ is multiplied with the square loss term so that the derivative function does not contain a factor. Overall, the problem investigated is based on iteratively minimizing this cost value, C. (Ng, 2012) 




\section{Introducing Vector Calculus}
The use of matrices that follow were largely introduced to me in grade 10 mathematics. I was inspired by the idea of involving my exploration with matrices, intertwined with applied calculus. So, I now introduce multivariate functions and calculus. When dealing with multivariate functions, in which the output, say $f$, depends on more than one variable and not just $x$, it is appropriate to think of the derivative of a function in terms of vectors. When taking the derivative in terms of each variable at a time, the rate of change of the function is taken in terms of only one variable, while the other variables are considered constant. Such is expressed as a partial derivative, where derivatives are also to be taken in terms of other variables present in the function’s arguments (Dawkins 2018).

Starting with an example, first consider a (2 variable) function $f(x,y) = 2x^2+3y$. In this case, $f$ is dependent upon both variables x and $y$. If the partial derivative is taken in terms of $x$ first, then variable $y$ is considered a constant, while taking the derivative in terms of $x$ using the same principle of the power rule from single-variable calculus. Hence,


\begin{equation}
\frac{\partial}{\partial x}f\left(x,y\right)=4x+0=4x
\end{equation}


Note: $\frac{\partial}{\partial x}$ is the notation for the partial derivative, in terms of variable $x$.


Following the same method, taking the partial derivative in terms of $y$ requires the variable $x$ to be dealt as a constant. So,

\begin{equation}
\frac{\partial}{\partial y}f\left(x,y\right) = 0 + 3 = 3
\end{equation}


When considering the gradient of a bi-variate function, or $f(x,y)$, the following vector notation is utilized to denote the partial derivatives, or the rates of change in $f$ in terms of both dimensions: $x$ and $y$. The Jacobian expresses a multivariate function’s first-order partial derivatives in terms of all variables:

\begin{equation}
\nablaf\left(x,y\right)=\left[\begin{matrix}\frac{\partial}{\partial x}f\left(x,y\right)\\\frac{\partial}{\partial  y}f\left(x,y\right)\\\end{matrix}\right]\end{equation}


\begin{equation}
\therefore\ \nablaf\left(x,y\right)=\ \left[\begin{matrix}4x\\3\\\end{matrix}\right]
\end{equation}

What this vector gradient represents is the direction of steepest ascent, or where the output, $f$, increases most quickly in terms of both inputs. Visualising this vector gradient function, the following 3-dimensional graph represents the vector notation of the gradient function $\nabla f$. (Dawkins, 2018)


If a function $h$ has $n$ variables, such that $h(x_1, …, x_n)$, then its Jacobian, a matrix representing all first-order partial derivatives, will be expressed as,

\begin{equation}
\nablah\left(x_1,\ \ldots,\ x_n\right)=\ \left[\begin{matrix}\frac{\partial}{\partial x_1}h\left(x_1,\ \ldots,\ x_n\right)\\\vdots\\\frac{\partial}{\partial x_n}h\left(x_1,\ \ldots,\ x_n\right)\\\end{matrix}\right]
\end{equation}



\section{Regression Supervised Learning: A Network for Housing Prices}

\subsection{Looking at Data}


In this exploration, I use the available data of housing prices in Seattle, Washington from Kaggle, and thus use a model of a supervised, regression machine learning algorithm that models a multivariate function. Such relates to my own search for a future residence at university. In a training data set, the different variables are named as ‘features’ or inputs to a network of neurons that determines the values of the output. The graph in figure 4, however, is a set of 30 randomized points from the data in only a two dimensional plane. (Kaggle)

\begin{figure}[h]
    \centering
    \includegraphics{Picture155.png}
    \caption{A graph with non-linear regression}
    \label{fig:my_label}
\end{figure}


In this exploration, I use the available data of housing prices in Seattle, Washington (Kaggle), and thus use a model of a supervised, regression machine learning algorithm that models a multivariate function. Such relates to my own search for a future residence at university. In a training data set, the different variables are named as ‘features’ or inputs to a network of neurons that determines the values of the output. The graph in figure 4, however, is a set of 30 randomized points from the data in only a two dimensional plane. 

Logarithmic regression can be used to model the functions that represent the relationship between one variable, say the living area—as visualized in figure 4 (created using Excel)—with the selling price in dollars ($\$$). But when more than a single independent variable is introduced, such as lot size, number of bedrooms/bathrooms and living area for a data set with 30 examples, the trend cannot be visually represented after solving for the parameters of a function with 4 independent variables. Hence, using a trained neural network would be viable to input a vector of 4 features (variables) in the network so as to output a value that is later passed on to a cost function, that is minimized with each data point, or iteration. In other words, the weights and biases are the parameters for the learning function used for regression, which are updated in gradient descent after each training example.


\subsection{A Theoretical Neural Network to Model Continuous Values: Housing Prices}


The model of the neural network for regression will involve a single neuron in the output layer for a single output value, or the predicted housing price. The number of neurons in the input layer will be equal to the number of variables in the feature (input layer) vector. The network will involve a non-linear activation function on the hidden layer neurons, or neurons in layers between the inputs and output. Also for regression, the activation function will not be applied to the output layer neuron, since sigmoid functions are bounded on $(0,1)$, and thus do not possess ranges suitable to output larger predicted housing price values. In general, the hidden layers will be included to take into account the complexity of non-linear, multivariate regression using neural nets in machine learning algorithms, where the activation functions apply to the weighted sums. (Ng, 2012) 


Furthermore, as detailed by a paper from Columbia University, the process starts with “weights randomly initialized and then adjusted in many small steps to bring the network closer to the desired behavior.” (Kriegeskorte, 2019) In figure 7, there is one hidden layer for the feed-forward, shallow artificial neural network. The hidden layer is included since the activation functions on the neurons in this layer have linear weighted sums (expressed as  $a^{(L-1)}(w^L)+b$) followed by a ‘squashing’ non-linearity to result in the activations accounting for non-linear regression. The single hidden layer network is able to “approximate any function that contains a continuous mapping from one finite space to another", or features to labels. Also, the number of hidden layer neurons is 3, between the number of input and output neurons for clarity. I established the following setup of the synapses and neurons below:

\begin{figure}[h]
    \centering
    \includegraphics[width=0.95\textwidth]{Screen Shot 2021-01-04 at 9.43.35 AM.png}
    \caption{Neural Network Model. This visualization was created using “alexlenail”.}
    \label{fig:my_label}
\end{figure}



\subsection{Backpropagation for Gradient Descent with an Activation Function}
For each iteration, supervised learning is used to ‘train’ the neural net. The purpose is to minimize the cost function in stochastic (random) gradient descent, in which the backpropagation procedure evaluates the gradients of the cost function. In stochastic gradient descent, the batch size, or the number of examples per forward and backward pass that defines one iteration, is one. In each iteration, the weights and biases values are updated. The features are the inputs of forward propagation before calculating the cost. Then, the first partial derivatives of the cost function are calculated and represented as a matrix (Jacobian) that, with respect to each weight and bias, provides both the magnitude and direction (sign) of change to update the weights and biases (parameters) in each iteration. (Rocca, 2018) Hence, backpropagation is essentially using matrix derivatives that express, using the chain rule, the cost function as the composition of the function in terms of the weights and biases. It is important to realize that the output neuron’s activations depend on the value of the features.





\section{The Neural Network’s Functions}

\subsection{Deriving Equations for Forward Propagation:}
The sigmoid function allows for ‘logistic regression’ to take place within the hidden layers using the non-linear activation function to model an unknown function for the relationship between all the variables and housing prices. Each element of the matrices (array of numbers) represent either a parameter or activation to or from the neurons in each layer. The following equations, with the sigmoid activation function applied to the weighted sum to layer (1) neurons, will later be used to find the first partial derivatives useful in backpropagation. For forward propagation, the process of finding the network’s output, $a_0^{\left(2\right)}$, employs the following equation involving all neurons in each layer. In terms of all of the weights and biases to first solve for the outputs, or activations of layer (1) neurons, the following matrix equation for forward propagation from layer (0) to (1) is:

\begin{equation}
a^{\left(1\right)}= w^{(1)} \cdot a^{(0)}+b^{(1)}
\end{equation}


\begin{equation}
a^{\left(1\right)}=\left[\begin{matrix}a_0^{\left(1\right)}\\a_1^{\left(1\right)}\\a_2^{\left(1\right)}\\\end{matrix}\right] = \sigma\left(\left[\begin{matrix}w_{00}^{\left(1\right)}a_0^{\left(0\right)}+w_{01}^{\left(1\right)}a_1^{\left(0\right)}+w_{02}^{\left(1\right)}a_2^{\left(0\right)}+w_{03}^{\left(1\right)}a_3^{\left(0\right)}\\w_{10}^{\left(1\right)}a_0^{\left(0\right)}+w_{11}^{\left(1\right)}a_1^{\left(0\right)}+w_{12}^{\left(1\right)}a_2^{\left(0\right)}+w_{13}^{\left(1\right)}a_3^{\left(0\right)}\\w_{20}^{\left(1\right)}a_0^{\left(0\right)}+w_{21}^{\left(1\right)}a_1^{\left(0\right)}+w_{22}^{\left(1\right)}a_2^{\left(0\right)}+w_{23}^{\left(1\right)}a_3^{\left(0\right)}\\\end{matrix}\begin{matrix}{+\ b}_0^{\left(1\right)}\\+\ b_1^{\left(1\right)}\\+\ b_2^{\left(1\right)}\\\end{matrix}\right]\right)
\end{equation}



However, the sigmoid activation function is not applied to the output layer, which is a continuous value of housing price. A linear “activation” function is instead used, such that the output, $a_0^{\left(2\right)}=z^{\left(2\right)}$, where $z^{\left(2\right)}$ is the weighted sum to the output layer neuron. Thus, the forward propagation equation to the layer (2) neuron is:


\begin{equation}
a^{\left(2\right)}=w^{(2)} \cdot a^{(1)}+b^{(2)}
a^{(2)} = \left(\left[\begin{matrix}w_{00}^{\left(2\right)}&w_{01}^{\left(2\right)}&w_{02}^{\left(2\right)}\\\end{matrix}\right]\left[\begin{matrix}a_0^{\left(1\right)}\\a_1^{\left(1\right)}\\a_2^{\left(1\right)}\\\end{matrix}\right]+\ \left[b_0^{\left(2\right)}\right]\right)
\end{equation}

\begin{equation}
\left[a_0^{\left(2\right)}\right]=\left(\left[w_{00}^{\left(2\right)}a_0^{\left(1\right)}+w_{01}^{\left(2\right)}a_1^{\left(1\right)}+w_{02}^{\left(2\right)}a_2^{\left(1\right)}+b_0^{\left(2\right)}\right]\right)
\end{equation}

\subsection{Deriving Equations for Backpropagation:}
To correlate with my neural network, I derived the following equations, extending on the models for basic neural networks researched. First, the partial derivative in terms of a single weight is considered, which will be a part of the matrix-vector that represents the overall cost function for backpropagation used for 30 examples/iterations (30 houses). The cost function C(\ldots) is defined in terms of all the weights and biases present in the neural network. (Urtasun et al., 2015) Hence, the cost function is defined as: 

\begin{equation}
C\left(w_{00}^{\left(1\right)},\ldots,w_{jk}^{\left(1\right)},\ w_{0j}^{\left(2\right)},b_j^{\left(1\right)},b_0^{\left(2\right)}\right)={\frac{1}{2}\left(a_0^{\left(2\right)}\ -\ y\right)}^2\ 
\end{equation}


 To represent such with matrices, the \textbf{Jacobian} shows all the values of the first partial derivatives of the cost function with respect to each weight and bias. The number of partial derivatives is equal to the total number of weights and biases. The cost function is defined in terms of 4 matrices, for the weights and biases between each of the 3 layers.  The Jacobians are:
 
\begin{equation}
    \frac{\partial C}{\partial\mathbit{w}^{\left(\mathbf{2}\right)}}=\ \left[\begin{matrix}\frac{\partial C}{\partial\mathbit{w}_{\mathbf{00}}^{\left(\mathbf{2}\right)}}\\\frac{\partial C}{\partial\mathbit{w}_{\mathbf{01}}^{\left(\mathbf{2}\right)}}\\\frac{\partial C}{\partial\mathbit{w}_{\mathbf{02}}^{\left(\mathbf{2}\right)}}\\\end{matrix}\right]
\end{equation}


\begin{equation}
\frac{\partial C}{\partial\mathbit{w}^{\left(\mathbf{1}\right)}}=\ \left[\begin{matrix}\frac{\partial C}{\partial\mathbit{w}_{\mathbf{00}}^{\left(\mathbf{1}\right)}}&\frac{\partial C}{\partial\mathbit{w}_{\mathbf{01}}^{\left(\mathbf{1}\right)}}&\frac{\partial C}{\partial\mathbit{w}_{\mathbf{02}}^{\left(\mathbf{1}\right)}}\\\frac{\partial C}{\partial\mathbit{w}_{\mathbf{10}}^{\left(\mathbf{1}\right)}}&\frac{\partial C}{\partial\mathbit{w}_{\mathbf{11}}^{\left(\mathbf{1}\right)}}&\frac{\partial C}{\partial\mathbit{w}_{\mathbf{12}}^{\left(\mathbf{1}\right)}}\\\frac{\partial C}{\partial\mathbit{w}_{\mathbf{20}}^{\left(\mathbf{1}\right)}}&\frac{\partial C}{\partial\mathbit{w}_{\mathbf{21}}^{\left(\mathbf{1}\right)}}&\frac{\partial C}{\partial\mathbit{w}_{\mathbf{22}}^{\left(\mathbf{1}\right)}}\\\end{matrix}
\qquad
\begin{matrix}\frac{\partial C}{\partial\mathbit{w}_{\mathbf{03}}^{\left(\mathbf{1}\right)}}\\\frac{\partial C}{\partial\mathbit{w}_{\mathbf{13}}^{\left(\mathbf{1}\right)}}\\\frac{\partial C}{\partial\mathbit{w}_{\mathbf{23}}^{\left(\mathbf{1}\right)}}\\\end{matrix}\right]	
\qquad
\frac{\partial C}{\partial\mathbit{b}^{\left(\mathbf{2}\right)}}=\left[\frac{\partial C}{\partial\mathbit{b}_\mathbf{0}^{\left(\mathbf{2}\right)}}\right]
\end{equation}

\begin{equation}
\frac{\partial C}{\partial\mathbit{b}^{\left(\mathbf{1}\right)}}=\left[\begin{matrix}\frac{\partial C}{\partial\mathbit{b}_\mathbf{0}^{\left(\mathbf{2}\right)}}\\\frac{\partial C}{\partial\mathbit{b}_\mathbf{1}^{\left(\mathbf{2}\right)}}\\\frac{\partial C}{\partial\mathbit{b}_\mathbf{2}^{\left(\mathbf{2}\right)}}\\\end{matrix}\right]
\end{equation}



These derivatives, in terms of each weight and bias in the network, may be thought of as the representative vectors in a space with 4 input variables and one output (house price), and thus 5 dimensions, where there is the steepest descent towards a minimum value of the cost function at a point in space; hence the name of “gradient descent”. Each single partial derivative term in each of these 4 Jacobians is solved for below.


\hspace{}

\textbf{Solving $\frac{\partial C}{\partial w^{\left(2\right)}}$∶ Weights from layer (1) to (2)}


In the cost function $C(…)$, the derivative of $y$ (actual housing price) with respect to $\w^{\left(2\right)}$ is 0 since it is a fixed value in an example from the data set used. To find the cost function’s derivative in terms of one of the weights from layer (1) to (2), or from neuron $j$ in layer (1) to neuron 0 in layer (2), the following chain rule using partial derivatives is applied:


\begin{equation}
\frac{\partial C}{\partial\mathbf{w}_{\mathbf{0}\mathbit{j}}^{\left(\mathbf{2}\right)}}=\frac{\partial C}{\partial\mathbit{a}_\mathbf{0}^{\left(\mathbf{2}\right)}}\frac{\partial\mathbit{a}_\mathbf{0}^{\left(\mathbf{2}\right)}}{\partial\mathbit{z}^{\left(\mathbf{2}\right)}}\frac{\partial\mathbit{z}^{\left(\mathbf{2}\right)}}{\partial\mathbit{w}_{\mathbf{0}\mathbit{j}}^{\left(\mathbf{2}\right)}}
\end{equation}	


\begin{equation}
\mathbf{C}\left(\ldots\right)= {\frac{\mathbf{1}}{\mathbf{2}}\left(\mathbf{a}_\mathbf{0}^{\left(\mathbf{2}\right)}\ -\ \mathbf{y}\right)}^\mathbf{2}
\end{equation}	
The mean square cost function is used after each single iteration.


\begin{equation}
\frac{\partial C}{\partial\mathbit{a}_\mathbf{0}^{\left(\mathbf{2}\right)}}={(\mathbit{a}}_\mathbf{0}^{\left(\mathbf{2}\right)}-\mathbit{y})
\end{equation}	
Derivative is taken for one training example.



\begin{equation}
\frac{\partial C}{\partial\mathbf{w}_{\mathbf{0j}}^{\left(\mathbf{2}\right)}}={(\mathbf{a}}_\mathbf{0}^{\left(\mathbf{2}\right)}-\mathbf{y})\frac{\partial\mathbf{a}_\mathbf{0}^{\left(\mathbf{2}\right)}}{\partial\mathbf{z}^{\left(\mathbf{2}\right)}}\frac{\partial\mathbf{z}^{\left(\mathbf{2}\right)}}{\partial\mathbf{w}_{\mathbf{0j}}^{\left(\mathbf{2}\right)}}	
\end{equation}	

In the derivative of the cost function, $C$, with respect to the weight, $w_{0j}^{\left(2\right)}$, the magnitude of the change in the cost function is proportional to the difference between the network’s output $a_0^{\left(2\right)}$ and the target value, $y$. Hence, if a training example’s prediction or activation on the output layer is nearer to the actual value, $y$, then the change in the parameters, or weights is smaller. Moreover, since the activation function is linear for the output layer as the values on the output are unbounded, the following is defined:


\begin{equation}
a_0^{\left(2\right)}=z^{\left(2\right)}=\left(\sum_{j=0}^{2}\left(w_{0j}^{\left(2\right)}\right)\left(a_j^{\left(1\right)}\right)\right)+\ b^{\left(2\right)}
\end{equation}	

\begin{equation}
    \frac{\partial C}{\partial\mathbf{w}_{\mathbf{0j}}^{\left(\mathbf{2}\right)}}={(\mathbf{a}}_\mathbf{0}^{\left(\mathbf{2}\right)}-\mathbf{y})\frac{\partial\mathbf{a}_\mathbf{0}^{(\mathbf{2})}}{\partial\mathbf{z}_\mathbf{0}^{\left(\mathbf{2}\right)}}\frac{\partial\mathbf{z}^{\left(\mathbf{2}\right)}}{\partial\mathbf{w}_{\mathbf{0j}}^{\left(\mathbf{2}\right)}}
\end{equation}

	
\begin{equation}
a_0^{\left(2\right)}= z^{\left(2\right)} 
\therefore \frac{\partial a_0^{(2)}}{\partial z_0^{\left(2\right)}} = 1.
\end{equation}	

	
	
\begin{equation}
\frac{\partial\mathbf{z}^{\left(\mathbf{2}\right)}}{\partial\mathbf{w}_{\mathbf{0}\mathbit{j}}^{\left(\mathbf{2}\right)}}=\mathbit{a}_\mathbit{j}^{\left(\mathbf{1}\right)}
\end{equation}	

In the summation expression above, the following derivative is taken in terms of a single weight only, so all other terms (from other layer (1) neurons) are constants.

\begin{equation}
\boxed{\therefore\frac{\partial C}{\partial w_{0j}^{\left(2\right)}}={(a}_0^{\left(2\right)}-y)(a_j^{\left(1\right)})}
\end{equation}	


\textbf{Solving $\frac{\partial C}{\partial b^{\left(2\right)}}$∶ Biases to layer (2)}


\begin{equation}
\frac{\partial C}{\partial b^{\left(2\right)}}=\frac{\partial C}{\partial a_0^{(2)}}\frac{\partial a_0^2}{\partial z_0^{\left(2\right)}}\frac{\partial z_0^{\left(2\right)}}{\partial b^{\left(2\right)}}
\end{equation}	


	
\begin{equation}
\frac{\partial C}{\partial a_0^{\left(2\right)}}={(a}_0^{\left(2\right)}-y)
\end{equation}	
The power rule is used on the same mean square cost function.


	
\begin{equation}
\frac{\partial C}{\partial b^{\left(2\right)}}={(a}_0^{\left(2\right)}-y)\frac{\partial  a_0^{\left(2\right)}}{\partial z^{\left(2\right)}}\frac{\partial z_0^{\left(2\right)}}{\partial b^{\left(2\right)}}	
\end{equation}


\begin{equation}
a_0^{\left(2\right)}=z^{\left(2\right)}=\left(\sum_{j=0}^{2}\left(w_{0j}^{\left(2\right)}\right)\left(a_j^{\left(1\right)}\right)\right)+\ b^{\left(2\right)}	
\end{equation} 
To layer (2) there is no activation function applied to the weighted sum, $z^{(2)}$. 




\begin{equation}
\frac{\partial C}{\partial b^{\left(2\right)}}={(a}_0^{\left(2\right)}-y)(1)\frac{\partial z_0^{\left(2\right)}}{\partial b^{\left(2\right)}}
\end{equation} 

\begin{equation}
	a_0^{\left(2\right)}= z^{\left(2\right)} \therefore \frac{\partial a_0^{(2)}}{\partial z_0^{\left(2\right)}} = 1.
\end{equation} 

\begin{equation}
\frac{\partial z_0^{\left(2\right)}}{\partial b^{\left(2\right)}}=1
\end{equation} 	

In solving for $\frac{\partial z_0^{\left(2\right)}}{\partial b^{\left(2\right)}}$, the term $\sum_{j=0}^{2}\left(w_{0j}^{\left(2\right)}\right)\left(a_j^{\left(1\right)}\right)$ is a constant, of which its derivative in terms of $b^{(2)}$ is 0. Hence,

\begin{equation}
\boxed{\therefore\frac{\partial C}{\partial b^{\left(2\right)}}={(a}_0^{\left(2\right)}-y)}
\end{equation}


\textbf{Solving $\frac{\partial C}{\partial w^{\left(1\right)}}$ : Weights from layer (0) to (1)}

In order to solve for the derivative in terms of $w_{jk}^{\left(1\right)}$, a single weight from neuron $k$ in layer (0) to neuron $j$ in layer (1), the multivariate chain rule is used. The cost function, $C$, is determined by the value of the output of the neural network, $a_0^{(2)}$, which is determined by the value of the weighted sum $z_0^{\left(2\right)}$, determined by the activation of one neuron in the previous layer. This activation, $a_j^{\left(1\right)}$, is determined by the weighted sum, $z_j^{\left(1\right)}$ to the neuron that is linked with the single weight the partial derivative is taken in terms of.


\begin{equation}
\frac{\partial C}{\partial\mathbf{w}_{\mathbit{jk}}^{\left(\mathbf{1}\right)}}=\frac{\partial C}{\partial\mathbf{a}_\mathbf{0}^{(\mathbf{2})}}\frac{\partial\mathbf{a}_\mathbf{0}^{(\mathbf{2})}}{\partial\mathbf{z}_\mathbf{0}^{\left(\mathbf{2}\right)}}\frac{\partial\mathbf{z}_\mathbf{0}^{\left(\mathbf{2}\right)}}{\partial\mathbf{a}_\mathbit{j}^{\left(\mathbf{1}\right)}}\frac{\partial\mathbf{a}_\mathbit{j}^{\left(\mathbf{1}\right)}}{\partial\mathbf{z}_\mathbit{j}^{\left(\mathbf{1}\right)}}\frac{\partial\mathbf{z}_\mathbit{j}^{\left(\mathbf{1}\right)}}{\partial\mathbf{w}_{\mathbit{jk}}^{\left(\mathbf{1}\right)}}
\end{equation}
The multivariate chain rule is used.



\begin{equation}
\mathbit{a}_\mathbit{j}^{\left(\mathbf{1}\right)}=\ \mathbf{\sigma}\left(\mathbit{z}_\mathbit{j}^{\left(\mathbf{1}\right)}\right) = \mathbf{\sigma}\left(\left(\sum_{\mathbit{k}=\mathbf{0}}^{\mathbf{3}}\left(\mathbit{w}_{\mathbit{jk}}^{\left(\mathbf{1}\right)}\right)\left(\mathbit{a}_\mathbit{k}^{\left(\mathbf{0}\right)}\right)\right)+\mathbf{b}_\mathbit{j}^{\left(\mathbf{1}\right)}\right)	
\end{equation}
The weighted sum is the input of the sigmoid function to output the activation of neuron j in layer (1).



\begin{equation}
{\mathbf{a}_\mathbf{0}^{(\mathbf{2})}=\ \mathbit{z}}^{\left(\mathbf{2}\right)}=\left(\sum_{\mathbit{j}=\mathbf{0}}^{\mathbf{2}}\left(\mathbit{w}_{\mathbf{0}\mathbit{j}}^{\left(\mathbf{2}\right)}\right)\left(\mathbit{a}_\mathbit{j}^{\left(\mathbf{1}\right)}\right)\right)+\ \mathbf{b}^{\left(\mathbf{2}\right)}	
\end{equation}
The activation of the layer (2) neuron is the sum of the product of weights and activations from layer (1) neurons, added with a bias.



\begin{equation}
\frac{\partial C}{\partial\mathbf{w}_{\mathbit{jk}}^{\left(\mathbf{1}\right)}}={(\mathbit{a}}_\mathbf{0}^{\left(\mathbf{2}\right)}-\mathbit{y})(\mathbf{1})(\mathbit{w}_{\mathbf{0}\mathbit{j}}^{\left(\mathbf{2}\right)})\left(\mathbf{\sigma}\prime\left(\mathbit{z}_\mathbf{0}^{\left(\mathbf{1}\right)}\right)\right)(\mathbit{a}_\mathbit{k}^{\left(\mathbf{0}\right)})	
\end{equation}


\begin{equation}
\mathbf{\sigma}^\prime\left(\mathbit{z}_\mathbf{0}^{\left(\mathbf{1}\right)}\right)=\frac{\mathbf{e}^{-\mathbit{z}_\mathbit{j}^{\left(\mathbf{1}\right)}\ }}{\left(\mathbf{1}+\ \mathbf{e}^{-\mathbit{z}_\mathbit{j}^{\left(\mathbf{1}\right)}\ }\right)^\mathbf{2}}
\end{equation}
The derivative of the activation $a_j^{\left(1\right)}$ with respect to $z_j^{\left(1\right)}$ is the derivative of the sigmoid function, as calculated before. 
	
	
\begin{equation}
\boxed{\therefore\frac{\partial C}{\partial w_{jk}^{\left(1\right)}}={(a}_0^{\left(2\right)}-y)(w_{0j}^{\left(2\right)})\left(\frac{e^{-z_j^{\left(1\right)}\ }}{\left(1+e^{-z_j^{\left(1\right)}\ }\right)^2}\right)\ (a_k^{\left(0\right)})}
\end{equation}

\hspace{}

\textbf{Solving $\frac{\partial C}{\partial b^{\left(1\right)}}$∶ Biases to layer (1)}

The multivariate chain rule is similarly used to solve for the derivative of the cost function with respect to one of the three biases to layer (1) neurons. Hence,

\begin{equation}
\frac{\partial C}{\partial b_j^{\left(1\right)}}=\frac{\partial C}{\partial a_0^{(2)}}\frac{\partial a_0^{(2)}}{\partial z_0^{\left(2\right)}}\frac{\partial z_0^{\left(2\right)}}{\partial a_j^{\left(1\right)}}\frac{\partial a_j^{\left(1\right)}}{\partial z_j^{\left(1\right)}}\frac{\partial z_j^{\left(1\right)}}{\partial b_j^{\left(1\right)}}
\end{equation}


\begin{equation}
\mathbit{a}_\mathbit{j}^{\left(\mathbf{1}\right)}=\ \mathbf{\sigma}\left(\mathbit{z}_\mathbit{j}^{\left(\mathbf{1}\right)}\right)
\end{equation}

\begin{equation}
\mathbit{z}_\mathbit{j}^{\left(\mathbf{1}\right)}=\left(\sum_{\mathbit{k}=\mathbf{0}}^{\mathbf{3}}\left(\mathbit{w}_{\mathbit{jk}}^{\left(\mathbf{1}\right)}\right)\left(\mathbit{a}_\mathbit{k}^{\left(\mathbf{0}\right)}\right)\right)+\mathbf{b}_\mathbit{j}^{\left(\mathbf{1}\right)}	
\end{equation}

The activation on each layer (1) neuron is the output of the sigmoid function with an input of the weighted sum, $\mathbit{z}_\mathbit{j}^{\left(\mathbf{1}\right)}$, to the neuron concerned.


\begin{equation}
\mathbf{z}_\mathbf{0}^{\left(\mathbf{2}\right)}=\mathbit{z}^{\left(\mathbf{2}\right)}=\left(\sum_{\mathbit{j}=\mathbf{0}}^{\mathbf{2}}\left(\mathbit{w}_{\mathbf{0}\mathbit{j}}^{\left(\mathbf{2}\right)}\right)\left(\mathbit{a}_\mathbit{j}^{\left(\mathbf{1}\right)}\right)\right)+\ \mathbf{b}^{\left(\mathbf{2}\right)}	
\end{equation}

This is the weighted sum to the layer (2) neuron.


\begin{equation}
\frac{\partial\mathbf{z}_\mathbit{j}^{\left(\mathbf{1}\right)}}{\partial\mathbf{b}_\mathbit{j}^{\left(\mathbf{1}\right)}}=\mathbf{1}
\end{equation}


The derivative of $z_j^{\left(1\right)}$ with respect to a single bias to a layer 1 neuron is 1, since the weighted sum excluding the bias is a constant.


\begin{equation}
\frac{\partial C}{\partial\mathbf{b}_\mathbit{j}^{\left(\mathbf{1}\right)}}={(\mathbit{a}}_\mathbf{0}^{\left(\mathbf{2}\right)}-\mathbit{y})(\mathbf{1})(\mathbit{w}_{\mathbf{0}\mathbit{j}}^{\left(\mathbf{2}\right)})(\mathbf{\sigma}^\prime\left(\mathbf{z}^{\left(\mathbf{1}\right)}\right))(\mathbf{1})	
\end{equation}


\begin{equation}
\boxed{\therefore\frac{\partial C}{\partial\mathbf{b}_\mathbit{j}^{\left(\mathbf{1}\right)}}={(\mathbit{a}}_\mathbf{0}^{\left(\mathbf{2}\right)}-\mathbit{y})(\mathbit{w}_{\mathbf{0}\mathbit{j}}^{\left(\mathbf{2}\right)})\left(\frac{\mathbit{e}^{-\mathbit{z}_\mathbit{j}^{\left(\mathbf{1}\right)}\ }}{\left(\mathbf{1}+\mathbit{e}^{-\mathbit{z}_\mathbit{j}^{\left(\mathbf{1}\right)}\ }\right)^\mathbf{2}}\right)}
\end{equation}


$\sigma^\prime\left(z^{\left(1\right)}\right)$ is calculated before. 







\subsection{Training Example 1}

\subsubsection{Forward Propagation with Example 1:}

The first example from the training data may be used to first calculate the output of the network using forward propagation, before using backwards propagation to calculate the updated weights that result in a lower cost value for subsequent iterations. The weights and biases are initialized with random values. The following numbers were the outputs of the code written in Python 3.0.



\begin{verbatim}
import random
W = round(random.uniform(0, 0.001),10)
print(W)
 
\end{verbatim}




\newtcolorbox{mybox2}[1]{colback=red!5!white,colframe=red!75!black}

\begin{mybox2}

The weights and biases were randomized in the range (0, 0.001) as in the sigmoid activation function, the gradient is close to zero when z, the weighted sum, is large. All the randomized initial weights and biases are:

\begin{equation}
\left[\begin{matrix}\mathbit{b}_\mathbf{0}^{\left(\mathbf{1}\right)}\\\mathbit{b}_\mathbf{1}^{\left(\mathbf{1}\right)}\\\mathbit{b}_\mathbf{2}^{\left(\mathbf{1}\right)}\\\end{matrix}\right]=\ \left[\begin{matrix}\mathbf{0}.\mathbf{0003656608}\\\mathbf{0}.\mathbf{0007430702}\\\ \mathbf{0}.\mathbf{0004790847}\\\end{matrix}\right]
	\left[\mathbit{b}_\mathbf{0}^{\left(\mathbf{2}\right)}\right]=\left[\mathbf{0}.\mathbf{0007412303}\right]
\end{equation}

\begin{equation}
\left[\begin{matrix}w_{00}^{\left(1\right)}&w_{01}^{\left(1\right)}&w_{02}^{\left(1\right)}\\w_{10}^{\left(1\right)}&w_{11}^{\left(1\right)}&w_{12}^{\left(1\right)}\\w_{20}^{\left(1\right)}&w_{21}^{\left(1\right)}&w_{22}^{\left(1\right)}\\\end{matrix}\ \ \ \ \begin{matrix}w_{03}^{\left(1\right)}\\w_{13}^{\left(1\right)}\\w_{23}^{\left(1\right)}\\\end{matrix}\right]=\ \left[\begin{matrix}0.0001579&0.0006046&0.0001568\\0.0004433&0.0005851&0.0007728\\0.0003029&0.0001045&0.0009119\\\end{matrix}\ \ \ \ \begin{matrix}0.0009584\\0.0004575\\0.0002420\\\end{matrix}\right]
\end{equation}

\begin{equation}
\left[\begin{matrix}w_{00}^{\left(2\right)}&w_{01}^{\left(2\right)}&w_{02}^{\left(2\right)}\\\end{matrix}\right]=\left[\begin{matrix}0.0007504129&0.0004124064&0.0006434057\\\end{matrix}\right]
\end{equation}

\end{mybox2}


The feature vector, or the input variables of the first example house to the input layer is written as:

\begin{equation}
    \left[\begin{matrix}a_0^{(0)}\\a_1^{(0)}\\a_2^{(0)}\\ a_3^{(0)}\\\end{matrix}\right] = \left[\begin{matrix}lot \ size \ (ft^2) \\ number \  of \  bedrooms \\ number \ of \ bathrooms \\ living \ area \ (ft^2) \\\end{matrix}\right] = \left[\begin{matrix} 11200 \\  4 \\ 2.5 \\ 2180 \\\end{matrix}\right]
\end{equation}


\hspace{}

\textbf{Layer 1 Operations (activations)}

\textit{Note: The values calculated from here are noted to 3 decimal places. The values are stored in the GDC and are not rounded. The quoted values for the prices ($\$$) are rounded to 2 decimal places. The following computations are solved for on the TI-84 GDC.}


\begin{equation}
\left[\begin{matrix}a_0^{\left(1\right)}\\a_1^{\left(1\right)}\\a_2^{\left(1\right)}\\\end{matrix}\right]=\sigma\left(\left[\begin{matrix}w_{00}^{\left(1\right)}a_0^{\left(0\right)}+w_{01}^{\left(1\right)}a_1^{\left(0\right)}+w_{02}^{\left(1\right)}a_2^{\left(0\right)}+w_{03}^{\left(1\right)}a_3^{\left(0\right)}\\w_{10}^{\left(1\right)}a_0^{\left(0\right)}+w_{11}^{\left(1\right)}a_1^{\left(0\right)}+w_{12}^{\left(1\right)}a_2^{\left(0\right)}+w_{13}^{\left(1\right)}a_3^{\left(0\right)}\\w_{20}^{\left(1\right)}a_0^{\left(0\right)}+w_{21}^{\left(1\right)}a_1^{\left(0\right)}+w_{22}^{\left(1\right)}a_2^{\left(0\right)}+w_{23}^{\left(1\right)}a_3^{\left(0\right)}\\\end{matrix}\begin{matrix}{+\ b}_0^{\left(1\right)}\\+\ b_1^{\left(1\right)}\\+\
b_2^{\left(1\right)}\\\end{matrix}\right]\right)=\sigma\left(\left[\begin{matrix}3.861...\\5.966...\\3.923...\\\end{matrix}\right]\right)
\end{equation}

Here, the sigmoid function is applied:

\begin{equation}
\left[\begin{matrix}a_0^{\left(1\right)}\\a_1^{\left(1\right)}\\a_2^{\left(1\right)}\\\end{matrix}\right]=\left[\begin{matrix}0.979...\\0.997...\\0.980...\\\end{matrix}\right]
\end{equation}

\hspace{}

\textbf{Layer 2 Operations (activation, neural net’s output)}


\begin{equation}
\left[a_0^{\left(2\right)}\right]=\left(\left[w_{00}^{\left(2\right)}a_0^{\left(1\right)}+w_{01}^{\left(2\right)}a_1^{\left(1\right)}+w_{02}^{\left(2\right)}a_2^{\left(1\right)}+b_0^{\left(2\right)}\right]\right)
\left[a_0^{\left(2\right)}\right]
\end{equation}


\begin{equation}
\left[a_0^{\left(2\right)}\right]= \$ 0.00251...\ \approx \$ 0.00
\end{equation}


Calculated above is the network’s predicted housing price based on the input features. As the network has not at all been ‘trained’ yet, the output value on the first iteration is far-off, in which the mean square cost value is large. 

\begin{equation}
C\left(\ldots\right)=\ {\frac{1}{2}\left(0.00251...\ -\ 480500\right)}^2=\ 1.15\times{10}^{11}\ 
\end{equation}
 

This cost value is used in backpropagation (below) to solve for the partial derivatives used to update weights and biases

\subsubsection{Backpropagation with Example 1}

In this neural network, the weights and biases are updated after each iteration which consists of a single training example. 



\hspace{}

\textbf{Backpropagating in terms of the weights to layer (2)} 
\begin{multicols}{2}
\begin{figure}[H]
    \centering
    \includegraphics[width=0.4\textwidth]{Screen Shot 2021-01-04 at 9.19.58 AM.png}
    \caption{}
    \label{fig:my_label}
\end{figure} 
\columnbreak
In order to solve for the partial derivatives in terms of the weights to layer 2, I derived the following equation:

\begin{equation}
\frac{\partial C}{\partial\mathbf{w}_{\mathbf{0}\mathbit{j}}^{\left(\mathbf{2}\right)}}={(\mathbit{a}}_\mathbf{0}^{\left(\mathbf{2}\right)}-\mathbit{y})(\mathbit{a}_\mathbit{j}^{\left(\mathbf{1}\right)})
\end{equation}
\end{multicols}


Here, $a_0^{\left(2\right)}$ is the output neuron’s activation (prediction for housing price ($\$$)) for the first training example, where the desired output is $y$. $a_j^{\left(1\right)}$ represents the activation, or output of a single neuron $j$ in layer (1). This partial derivative is solved with respect to the three weights to layer (2), with j = 0, 1, 2 in the following computations.

\begin{equation}
\frac{\partial C}{\partial w^{\left(2\right)}}=\ \left[\begin{matrix}\frac{\partial C}{\partial w_{00}^{\left(2\right)}}\\\frac{\partial C}{\partial w_{01}^{\left(2\right)}}\\\frac{\partial C}{\partial w_{02}^{\left(2\right)}}\\\end{matrix}\right]=\left[\begin{matrix}-470597.132...\\-479271.935...\\-471180.267...\\\end{matrix}\right] 
\end{equation}

\begin{equation}
\frac{\partial C}{\partial w^{\left(2\right)}}=\ \left[\begin{matrix}\frac{\partial C}{\partial w_{00}^{\left(2\right)}}\\\frac{\partial C}{\partial w_{01}^{\left(2\right)}}\\\frac{\partial C}{\partial w_{02}^{\left(2\right)}}\\\end{matrix}\right]^T=\left[\begin{matrix}-470597.132...&-479271.935...&-471180.267\\\end{matrix}...\right] 
\end{equation}

\textit{Note: The Jacobian matrix is transposed (rows and columns exchange), as represented by $T$, so that the dimensions of the matrix $(1 \times 3)$ are the same as that of the matrix of weights for $w^{(2)}$.}

\hspace{}

\textbf{Updating Weights to Layer (2)}

After I calculated the partial derivatives with respect to each of the weights (the rates of changes of the cost function), the weights are updated using the following equations with respect to each weight in the matrix. In using an update function, a learning rate ($\eta$)—a constant multiplied by the cost function’s derivative with respect to a weight—would update the network’s weights. Accordingly, a research paper on Deep Learning by Quoc V states that learning rates from 0.01 to 0.1 are a “very good start”. (Le, 2015) The updates for each weight, \textbf{using a learning rate of 0.01}—a lower rate is used to prevent the model from overshooting the optimal weights—are carried out with the following formula adapted from Le’s paper, with appropriate notations:


\newtcolorbox{mybox}[1]{colback=blue!5!white,colframe=blue!75!black}
\begin{mybox}

\begin{equation}
{\acute{w}}_{\left(0j\right)}^{\left(2\right)}=\ w_{0j}^{\left(2\right)}-\left(\eta\frac{\partial C}{\partial w_{0j}^{\left(2\right)}}\right)
\end{equation}


\begin{equation}
{\acute{w}}_{0j}^{\left(\mathbf{2}\right)}=any\ single\ updated\ weight\ from\ layer\ (1)\ \mathbit{to}\ \mathbit{layer}\ \left(\mathbf{2}\right).
\end{equation}


Hence,
\begin{equation}
{\acute{w}}^{\left(2\right)}= \left[\begin{matrix}w_{00}^{\left(2\right)}-\left(\eta\frac{\partial C}{\partial w_{00}^{\left(2\right)}}\right)&w_{01}^{\left(2\right)}-\left(\eta\frac{\partial C}{\partial w_{01}^{\left(2\right)}}\right)&w_{02}^{\left(2\right)}-\left(\eta\frac{\partial C}{\partial w_{02}^{\left(2\right)}}\right)\\\end{matrix}\right]
\end{equation}


\begin{equation}
{\acute{\mathbit{w}}}^{\left(\mathbf{2}\right)}=\left[\begin{matrix}\mathbf{4705}.\mathbf{972}...&\mathbf{4792}.\mathbf{719}...&\mathbf{4711}.\mathbf{803}...\\\end{matrix}\right]
\end{equation}

\textit{Note: These updated weights would only apply to the second iteration (using the second example).}
\end{mybox}

\hspace{}

\textbf{Backpropagating in terms of the bias to layer (2)}

\begin{equation}
\frac{\partial C}{\partial b_0^{\left(2\right)}}={(a}_0^{\left(2\right)}-y) =[-480499.997...]
\end{equation}



\newtcolorbox{mybox1}[1]{colback=blue!5!white,colframe=blue!75!black,fonttitle=\bfseries,title=#1}
\begin{mybox1}{Updating Biases to Layer (2)}


\begin{equation}
{\acute{b}}_0^{\left(2\right)}\ =\ b_0^{\left(2\right)}-\left(\eta\frac{\partial C}{\partial b_0^{\left(2\right)}}\right)
{\acute{\mathbit{b}}}_\mathbf{0}^{\left(\mathbf{2}\right)}=\left[\mathbf{4805}.\mathbf{000}...\right] 
\end{equation}

\end{mybox1}


\textbf{Backpropagating in terms of the weights to layer (1)}
\begin{multicols}{2}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.35\textwidth]{Screen Shot 2021-01-04 at 9.33.46 AM.png}
    \caption{}
    \label{fig:my_label}
\end{figure}


\columnbreak

\begin{equation}
\frac{\partial C}{\partial w_{jk}^{\left(1\right)}}={(a}_0^{\left(2\right)}-y)(w_{0j}^{\left(2\right)})\left(\sigma^\prime\left(z_0^{\left(1\right)}\right)\right)(a_k^{\left(0\right)})
\end{equation}

The weight $w_{0j}^{\left(2\right)}$, is taken from the initial randomized weights to layer (2), and not from the updated weights above. Updated weights will be used in the forward pass using Example 2.

Solving $\frac{\partial C}{\partial w_{00}^{\left(1\right)}},$ with $j=0, k=0$ (neuron 0 in layer 0 to neuron 0 in layer 1):
\begin{equation}
\frac{\partial C}{\partial w_{00}^{\left(1\right)}}={(a}_0^{\left(2\right)}-y)\left(w_{00}^{\left(2\right)}\right)\left(\sigma^\prime\left(z_0^{\left(1\right)}\right)\right)(a_0^{\left(0\right)})
\end{equation}

\end{multicols}


The weighted sum (summation of the product of layer 1 neuron activations, or the features of the house, and weights) to layer 1 is:

\begin{equation}
z_0^{\left(1\right)}=a_0^{\left(0\right)}w_{00}^{\left(1\right)}+a_1^{\left(0\right)}w_{01}^{\left(1\right)}+a_2^{\left(0\right)}w_{02}^{\left(1\right)}+a_3^{\left(0\right)}w_{03}^{\left(1\right)}+b_0^{\left(1\right)}=3.861... 
\end{equation}

The derivative of the sigmoid function with the weighted sum, $z_0^{\left(1\right)}$ is:



\begin{equation}
\sigma^\prime\left(z_0^{\left(1\right)}\right)=\frac{e^{-z_0^{\left(1\right)}}}{\left(1+e^{-z_0^{\left(1\right)}}\right)^2}=\ 0.020...
\end{equation}


\begin{equation}
    \frac{\partial C}{\partial w_{00}^{\left(1\right)}} =\left(0.00251...-\ 480500\right)\left(0.000750...\right)\left(0.020...\right)\left(11200\right)=\ -81514.552...
\end{equation}

The 11 other partial derivatives in the matrix of partial derivatives below with respect to all the weights between layers (0) to (1) are calculated repeating the same method. Hence,

\begin{equation}
\frac{\partial C}{\partial w^{\left(1\right)}}=\ \left[\begin{matrix}\frac{\partial C}{\partial w_{00}^{\left(1\right)}}&\frac{\partial C}{\partial w_{01}^{\left(1\right)}}&\frac{\partial C}{\partial w_{02}^{\left(1\right)}}\\\frac{\partial C}{\partial w_{10}^{\left(1\right)}}&\frac{\partial C}{\partial w_{11}^{\left(1\right)}}&\frac{\partial C}{\partial w_{12}^{\left(1\right)}}\\\frac{\partial C}{\partial w_{20}^{\left(1\right)}}&\frac{\partial C}{\partial w_{21}^{\left(1\right)}}&\frac{\partial C}{\partial w_{22}^{\left(1\right)}}\\\end{matrix}\ \ \ \ \begin{matrix}\frac{\partial C}{\partial w_{03}^{\left(1\right)}}\\\frac{\partial C}{\partial w_{13}^{\left(1\right)}}\\\frac{\partial C}{\partial w_{23}^{\left(1\right)}}\\\end{matrix}\right]\ =\ \left[\begin{matrix}-81514.552&-29.112&-18.195\\-5657.864&-15.999&-1.262\\-65856.593&-23.520&-14.700\\\end{matrix}\ \ \ \ \begin{matrix}-15866.225\\-1101.262\\-12818.515\\\end{matrix}\right]
\end{equation}


These all represent the factor of the changes to each of the weights represented in this Jacobian matrix.

\hspace{}


\begin{mybox1}{Updated Weights to Layer (1):}


${\acute{w}}_{jk}^{\left(1\right)}=new,\ updated\ weights\ from\ layer\ \left(1\right)\ to\ \left(2\right).$

Maintaining the same learning rate of \textbf{$\eta = 0.01$}, the following updates to the weights are applied:

\begin{equation}
{\acute{w}}^{\left(1\right)}=\left[\begin{matrix}w_{00}^{\left(1\right)}-\left(\eta\frac{\partial C}{\partial w_{00}^{\left(1\right)}}\right)&w_{01}^{\left(1\right)}-\left(\eta\frac{\partial C}{\partial w_{01}^{\left(1\right)}}\right)&w_{02}^{\left(1\right)}-\left(\eta\frac{\partial C}{\partial w_{02}^{\left(1\right)}}\right)\\w_{10}^{\left(1\right)}-\left(\eta\frac{\partial C}{\partial w_{10}^{\left(1\right)}}\right)&w_{11}^{\left(1\right)}-\left(\eta\frac{\partial C}{\partial w_{11}^{\left(1\right)}}\right)&w_{12}^{\left(1\right)}-\left(\eta\frac{\partial C}{\partial w_{12}^{\left(1\right)}}\right)\\w_{20}^{\left(1\right)}-\left(\eta\frac{\partial C}{\partial w_{20}^{\left(1\right)}}\right)&w_{21}^{\left(1\right)}-\left(\eta\frac{\partial C}{\partial w_{21}^{\left(1\right)}}\right)&w_{22}^{\left(1\right)}-\left(\eta\frac{\partial C}{\partial w_{22}^{\left(1\right)}}\right)\\\end{matrix}\ \ \ \ \begin{matrix}w_{03}^{\left(1\right)}-\left(\eta\frac{\partial C}{\partial w_{03}^{\left(1\right)}}\right)\\w_{13}^{\left(1\right)}-\left(\eta\frac{\partial C}{\partial w_{13}^{\left(1\right)}}\right)\\w_{23}^{\left(1\right)}-\left(\eta\frac{\partial C}{\partial w_{23}^{\left(1\right)}}\right)\\\end{matrix}\right]
\end{equation}

\begin{equation}
{\acute{\mathbit{w}}}^{\left(\mathbf{1}\right)}=\left[\begin{matrix}\mathbf{815}.\mathbf{145}...&\mathbf{0}.\mathbf{2917}...&\mathbf{0}.\mathbf{182}...\\\mathbf{56}.\mathbf{579}...&\mathbf{0}.\mathbf{160}...&\ \mathbf{0}.\mathbf{013}...\\\mathbf{658}.\mathbf{566}...&\mathbf{0}.\mathbf{235}...&\mathbf{0}.\mathbf{147}...\\\end{matrix}\ \ \ \ \begin{matrix}\mathbf{158}.\mathbf{663}...\\\mathbf{11}.\mathbf{013}...\ \\\mathbf{128}.\mathbf{185}...\\\end{matrix}\right]
\end{equation}

\end{mybox1}

\hspace{}

\textbf{Backpropagating in terms of the biases to layer (1)}

\begin{equation}
\frac{\partial C}{\partial\mathbit{b}^{\left(\mathbf{1}\right)}}=\left[\begin{matrix}\frac{\partial C}{\partial\mathbit{b}_\mathbf{0}^{\left(\mathbf{1}\right)}}\\\frac{\partial C}{\partial\mathbit{b}_\mathbf{1}^{\left(\mathbf{1}\right)}}\\\frac{\partial C}{\partial\mathbit{b}_\mathbf{2}^{\left(\mathbf{1}\right)}}\\\end{matrix}\right]
\end{equation}


\begin{equation}
	\frac{\partial C}{\partial\mathbf{b}_\mathbf{0}^{\left(\mathbf{1}\right)}}={(\mathbit{a}}_\mathbf{0}^{\left(\mathbf{2}\right)}-\mathbit{y})(\mathbit{w}_{\mathbf{00}}^{\left(\mathbf{2}\right)})\left(\mathbf{\sigma}\prime\left(\mathbf{z}_\mathbf{0}^{\left(\mathbf{1}\right)}\right)\right)
\mathbf{\sigma}^\prime\left(\mathbf{z}_\mathbf{0}^{\left(\mathbf{1}\right)}\right)=\ \left(\frac{\mathbit{e}^{-\mathbit{z}_\mathbf{0}^{\left(\mathbf{1}\right)}\ }}{\left(\mathbf{1}+\mathbit{e}^{-\mathbit{z}_\mathbf{0}^{\left(\mathbf{1}\right)}\ }\right)^\mathbf{2}}\right)\ \ =\ \mathbf{0}.\mathbf{020}...
\end{equation}

\begin{equation}
\frac{\partial C}{\partial\mathbf{b}_\mathbf{0}^{\left(\mathbf{1}\right)}}=-\mathbf{7}.\mathbf{278}...	
\end{equation}


Repeating the same method as used for the first partial derivative, the two other partial derivatives are:
\begin{equation}
\frac{\partial C}{\partial\mathbf{b}_\mathbf{1}^{\left(\mathbf{1}\right)}}=-\mathbf{0}.\mathbf{505}\ldots
\end{equation}

\begin{equation}
\frac{\partial C}{\partial\mathbf{b}_\mathbf{2}^{\left(\mathbf{1}\right)}}=\ -\mathbf{5}.\mathbf{880}...
\end{equation}


\begin{mybox1}{Updating Biases to Layer (1)}

${\acute{b}}_j^{\left(1\right)}=new,\ updated\ biases\ to\ layer\ \left(1\right)\ neurons.$

\begin{equation}
{\acute{\mathbit{b}}}^{\left(\mathbf{1}\right)}=\left[\begin{matrix}{\acute{\mathbit{b}}}_\mathbf{0}^{\left(\mathbf{1}\right)}\\{\acute{\mathbit{b}}}_\mathbf{1}^{\left(\mathbf{1}\right)}\\{\acute{\mathbit{b}}}_\mathbf{2}^{\left(\mathbf{1}\right)}\\\end{matrix}\right]=\left[\begin{matrix}\ \mathbit{b}_\mathbf{0}^{\left(\mathbf{1}\right)}-\left(\mathbf{\eta}\frac{\partial C}{\partial\mathbit{b}_\mathbf{0}^{\left(\mathbf{1}\right)}}\right)\\\ \mathbit{b}_\mathbf{1}^{\left(\mathbf{1}\right)}-\left(\mathbf{\eta}\frac{\partial C}{\partial\mathbit{b}_\mathbf{1}^{\left(\mathbf{1}\right)}}\right)\\\ \mathbit{b}_\mathbf{2}^{\left(\mathbf{1}\right)}-\left(\mathbf{\eta}\frac{\partial C}{\partial\mathbit{b}_\mathbf{2}^{\left(\mathbf{1}\right)}}\right)\\\end{matrix}\right] =\left[\begin{matrix}\mathbf{0}.\mathbf{073}...\\\mathbf{0}.\mathbf{005}...\\\mathbf{0}.\mathbf{059}...\\\end{matrix}\right]\ \
\end{equation}
\end{mybox1}



\subsection{Training Example 2}
\subsubsection{Forward Propagation with Example 2}


The house for the second example (feature vector) is:

\begin{equation}
    \left[\begin{matrix}a_0^{(0)} \\ a_1^{(0)} \\ a_2^{(0)} \\ a_3^{(0)} \end{matrix}\right]= \left[\begin{matrix} lot \ size \ (ft^2) \\ number \ of \  bedrooms \\ number \  of \ bathrooms \\ living \ area \ (ft^2) \end{matrix}\right] =  \left[\begin{matrix} 10050 \\  3  \\ 1  \\ 1060\end{matrix}\right]
\end{equation}

\textbf{Layer 1 Operations (activations)}

Using the updated weights and biases previously calculated, the method for solving for the estimated housing price in forward propagation is repeated in the following. The sigmoid function below has been applied to the new weighted sum. Moreover, the values of the activations to layer (1) in the matrix below approaching 1 as the weighted sums approach infinity are caused by ‘vanishing gradients’, or that the gradient approaches 0 as the weighted sum increases after each iteration.

\begin{equation}
\left[\begin{matrix}a_0^{\left(1\right)}\\a_1^{\left(1\right)}\\a_2^{\left(1\right)}\\\end{matrix}\right]=\ \left[\begin{matrix}1.00\\1.00\\1.00\\\end{matrix}\right]
\end{equation}

\hspace{}


\textbf{Layer 2 Operations (activation, neural net’s output)}

Again, the method used for forward propagation to the output layer’s activation is repeated this second iteration.

\begin{equation}
\left[a_0^{\left(2\right)}\right] \approx \$19015.50
\end{equation}

After the first iteration, where both forward and backpropagation was performed on one example, the value of the error decreases after the weights are adjusted, as for the second example’s cost function value. Note, however, that the effect of the different value of the target price in the second example compared to the first example on the cost value is evaluated in the conclusion. This cost value is to be used in backpropagation.

\begin{equation}
C\left(\ldots\right)= \frac{1}{2}\left(19015.495... - 335000\right)^2 =  4.99 \times{10}^{10}
\end{equation}


\section{Further Iterations:}

\subsection{Method and Improving the Model:}

Repeating the same method of applying the multivariate chain rule, the Keras library on Python integrated with TensorFlow was used. The code (as provided in Appendix A) used to train on the data set was modified considerably. The feature vector was instead scaled to values with the range $(0,1)$ so that, say the weights connected to the feature node with higher values (like the lot area) were updated on a similar scale of other features. Furthermore, the ReLU activation function was employed for neurons on both the hidden and output layers. As a piece-wise function, positive weighted sums preserve their values, while negative values are zero. The 'Adam' optimizer with a learning rate of 0.1 was chosen for more efficient learning. ()

Additionally, the data was split into a shuffled training and validation set. This distinction is made on the basis of optimizing parameter values and the model selection, respectively. ()

As detailed in the conclusion, the noise with the loss largely concerned with the former, simplified approach, is addressed by setting the batch size to be 15 rather than 1. This is the number of samples patched and processed at a time, before the weights and biases are updated. () Also, the number of epochs is 400, which is the number of times the model trains on the data by passing through all examples in the data set. 

\begin{figure}[h]
    \centering
    \includegraphics[width=0.85\textwidth]{Figure_544.png}
    \caption{Graph of loss against epochs in learning}
    \label{fig:my_label}
\end{figure}




\subsection{Interpretation of Data}

\section{Conclusion}

\subsection{Limitations and Reflection}

Thousands of training examples were used to train the neural network before feeding into the networks’ testing data (where weights and biases are fixed). Unlike the simplified example calculations iterating on single examples, multiple passes through training data are typically considered. With the Keras Tensorflow library, the averages of each batch were taken before the cost function’s average for each batch was considered for updating the weights/biases. (Urtusan et al., 2015) This method was used to address the ‘noisy’ results of stochastic gradient descent. (Le, 2015) Additionally, the computed values for the negative gradients and the local minimum value of the cost function may differ from the global minimum, leaving the possibility of a more accurate model to map features to labels. This was caused by the randomization of the values of weights and biases in the first example before backpropagation. 

With the finalized weights and biases over 30 training examples, my neural net achieved the aim to both compute the cost values, which on average, decreased with successive iterations. Thus, the network gradually increased its accuracy at predicting housing prices relative to the initial model (before the first iteration). The model, however, excluded other parameters such as location, age, and condition. Also, limited by the number of iterations, the model still wouldn’t accurately model the housing price of unseen housing data, with significant errors and “noise” explained above.

\subsection{Beyond the Exploration}
Software modules employing deep neural networks are rapidly emerging into various fields. The applications of linear algebra that stem from concepts of matrices and calculus coalesce as the backbone of AI, which vastly enhanced my knowledge and devotion for the wide-ranging field of data science. Software development may be utilized to include this search for homes and their prices in other cities, and similar ML models may be applied to data concerning a wide range of fields including medicine and computer vision.



























\newpage

\begin{thebibliography}{99}


\bibitem{mit} “33. Neural Nets and the Learning Function.” Lecture by Gilbert Strang, MIT OpenCourseWare, YouTube, 16 May 2019,  \url{www.youtube.com/watch?v=L3-WFKCW-tY.}



\bibitem{nyu} Choromanska, Anna, et al. “The Loss Surfaces of Multilayer Networks.” The Loss Surfaces of Multilayer Networks, NYU: Courant Institute of Mathematical Sciences, 2015,  \url{proceedings.mlr.press/v38/choromanska15.pdf.}

\bibitem{paul} Dawkins, Paul. “Calculus III - Chain Rule.” Partial Derivatives, 2018,  \url{tutorial.math.lamar.edu/Classes/CalcIII/ChainRule.aspx.}

\bibitem{heaton} Heaton, Jeff. The Number of Hidden Layers. Heaton Research, 1 June 2017,  \url{www.heatonresearch.com/2017/06/01/hidden-layers.html.}

\bibitem{columbia} Kriegeskorte, Nikolaus. Neural Network Models and Deep Learning – a Primer for Biologists. Columbia University, 2019,  \url{arxiv.org/pdf/1902.04704.pdf.}

\bibitem{andrew} Ng, Andrew. CS229 Lecture Notes. Stanford University, 2012, \url{ sgfin.github.io/files/notes/CS229_Lecture_Notes.pdf.}

\bibitem{rocca} Rocca, Joseph. A Gentle Journey from Linear Regression to Neural Networks. Towards Data Science, 13 July 2019, \url{ towardsdatascience.com/a-gentle-journey-from-linear-regression-to-neural-networks-68881590760e.}

\bibitem{kaggle} Shree. “House Price Prediction.” Kaggle, 26 Aug. 2018,  \url{www.kaggle.com/shree1992/housedata.}

\bibitem{life3.0} Tegmark, Max. Life 3.0: Being Human in the Age of Artificial Intelligence. Penguin Books, 2018.

\bibitem{uoft} Urtasun, Raquel, and Rich Zemel. CSC 411: Lecture 10: Neural Networks I. 2015, University of Toronto.




\end{thebibliography}




\begin{appendices}

\section{Code}
\begin{verbatim}import numpy as np 
import pandas as pd
import tensorflow as tf 
from tensorflow import keras
from tensorflow.keras.models import Sequential 
from tensorflow.keras.layers import Dense 
from sklearn.utils import shuffle
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn import preprocessing
from sklearn.model_selection import cross_val_score, KFold
from sklearn.pipeline import Pipeline
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense
from tensorflow.keras.wrappers.scikit_learn import KerasRegressor
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.metrics import categorical_crossentropy
from sklearn.model_selection import train_test_split
import matplotlib.pyplot as plt
import matplotlib as mpl
mpl.use('tkagg')

#physical_devices = tf.config.experimental.list_physical_devices('GPU')

# February 24, 2019


filename = 'data2444.csv'
names0 = ['sqft_lot','bedrooms','bathrooms','sqft_living']
#names00 = ['LotArea','OverallQual','BsmtFinSF1','BsmtFinSF2','BsmtUnfSF','TotalBsmtSF','1stFlrSF','2ndFlrSF','LowQualFinSF','GrLivArea','BsmtFullBath','BsmtHalfBath','FullBath','HalfBath','BedroomAbvGr','KitchenAbvGr','TotRmsAbvGrd','GarageCars','GarageArea','WoodDeckSF','OpenPorchSF','EnclosedPorch','3SsnPorch','ScreenPorch','PoolArea','MiscVal','MoSold','YrSold']
samples = np.array(pd.read_csv(filename, names=names0, skiprows=[0], header=None)).astype(float)

print(samples)
print(samples.shape)


where_are_NaNs = np.isnan(samples) # replaces NaNs with zeros 
samples[where_are_NaNs] = float(0)


names1 = ['price']
labels =
np.array(pd.read_csv(filename, names=names1, skiprows=[0], header=None)).astype(float)
where_are_NaNs = np.isnan(labels)
labels[where_are_NaNs] = float(0)
y = labels

print(labels.shape)


min_max_scaler = preprocessing.MinMaxScaler()
X = min_max_scaler.fit_transform(samples.astype(np.float))

X_train, X_test, y_train, y_test 
=train_test_split(X, y, test_size=0.2, random_state=1)
X_train, X_val, y_train, y_val =
train_test_split(X_train, y_train, test_size=0.2, random_state=1)

print(X_train)

print(samples.shape[1])
model = Sequential([
    Dense(units=3, input_shape=(samples.shape[1],), activation='relu'),
    Dense(units=1, activation='relu'),])
model.summary()

num_epochs = 400

model.compile(optimizer=Adam(learning_rate=0.1), loss='mse', metrics=['mae', 'mse'])

history = model.fit(X_train, y_train, epochs=num_epochs, batch_size=15, validation_data=(X_val, y_val), shuffle=True)


# PLOTTING:
loss_train = history.history['mae']
loss_val = history.history['val_mae']
rmse_final = np.sqrt(loss_train[-1])
print("Final Root Mean Squared Error = ", rmse_final)
epochs = np.arange(1,num_epochs+1)

plt.plot(epochs, loss_train, label='Training loss')
plt.plot(epochs, loss_val,label='Validation loss')

plt.title('Training and Validation loss')
plt.xlabel('Epochs')
plt.ylabel('Loss: Mean Squared Error [Dollars$^2$]')
plt.grid()
plt.legend()
plt.show()

\end{verbatim}

\end{appendices}

\end{document}
